# -*- coding: utf-8 -*-
"""MovieSentimentDL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iL34X0nbtOcZncLr14PhWhTFSRq94cy9
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import random
import spacy
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import models
from sklearn.metrics import confusion_matrix
import seaborn as sn

!python -m spacy download en_core_web_md

nlp = spacy.load("en_core_web_md")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Study/NLP

mcommentDF=pd.read_csv('imdb_sup.csv')
mcommentDF = mcommentDF[['Review','Sentiment']].dropna()
plt.figure(figsize=(5, 3.8))
axplot=mcommentDF.Sentiment.value_counts().plot(kind='bar')
plt.xlabel('labels')
plt.ylabel('Number of samples')
axplot.set_xticks(range(2))
axplot.set_xticklabels(['Negative', 'Positive'], rotation=0)
plt.grid()
plt.show()

mcommentDF.head()

mcommentDF['sent_len'] = mcommentDF['Review'].apply(lambda x: len(x.split(" ")))
max_seq_len = np.round(mcommentDF['sent_len'].mean() + 2 * mcommentDF['sent_len'].std()).astype(int)
max_seq_len

plt.figure(figsize=(5, 3.5))
mcommentDF['sent_len'].plot.hist()
plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len');
plt.title("Text length distribution")
plt.grid()

"""## **Training Data Preparation**

### **Pre-Processing**

***Punctuation Removal***
"""

import string
string.punctuation
#defining the function to remove punctuation
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree

mcommentDF['punc_remove']= mcommentDF['Review'].apply(lambda x:remove_punctuation(x))
mcommentDF.head()

"""***Lowering the text***"""

mcommentDF['lowering']= mcommentDF['punc_remove'].apply(lambda x: x.lower())

mcommentDF.head()

"""***tokenization***"""

def tokenization(text):
    mtoks = [token.text for token in nlp(text)]
    return mtoks

mcommentDF['tokennized']= mcommentDF['lowering'].apply(lambda x: tokenization(x))

mcommentDF.head()

"""***Remove stop words***"""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words

def remove_stopwords(text):
    output= [i for i in text if i not in stop_words]
    return output

mcommentDF['no_stopwords']= mcommentDF['tokennized'].apply(lambda x:remove_stopwords(x))

mcommentDF.head()

"""***Stemming***"""

from nltk.stem.porter import PorterStemmer
porter_stemmer = PorterStemmer()
def stemming(text):
    stem_text = [porter_stemmer.stem(word) for word in text]
    return stem_text

mcommentDF['stemmed']=mcommentDF['no_stopwords'].apply(lambda x: stemming(x))

mcommentDF.head()

"""***Lemmalization***"""

import nltk
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

def lemmatizer(text):
    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]
    return lemm_text

mcommentDF['lemmatized']=mcommentDF['no_stopwords'].apply(lambda x:lemmatizer(x))

mcommentDF.head()

"""***Save Pre-processed data***"""

test = mcommentDF.head()

file_name = 'imdb_preprocessed.csv'
mcommentDF.to_csv(file_name, sep=',', index=False)

"""***Read Processed data***

### ***Data Augmentation***
"""

from nltk.corpus import wordnet

def get_synonyms(word):
    synonyms = set()

    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
            synonyms.add(synonym)

    if word in synonyms:
        synonyms.remove(word)

    return list(synonyms)

"""***Replacing with synonyms***"""

aug_movie_comment_exp = []
aug_categories = []

for idx, rw in mcommentDF.iterrows():
    tokens = rw["stemmed"]
    rating = rw["Sentiment"]

    augmented_tokens = []

    for token in tokens:
        synonyms = get_synonyms(token)
        if synonyms:
            augmented_tokens.append(random.choice(synonyms))
        else:
            # If no synonyms found, keep the original token
            augmented_tokens.append(token)

    aug_movie_comment_exp.extend([tokens, augmented_tokens])
    aug_categories.extend([rating, rating])

augmented_df = pd.DataFrame({
    'augmented_data': aug_movie_comment_exp,
    'rating': aug_categories
})

# Save the DataFrame to a CSV file
augmented_df.to_csv('augmented_data.csv', sep=',', index=False)

print(np.array(aug_categories).shape)

"""### **Vectorize tokens and verified by IDs**

***Load data***
"""

aug_mcommentDF=pd.read_csv('imdb_filtered.csv')
aug_mcommentDF.head()

# extract review text and review label from each dataset row and add them into Python lists
movie_comment_aug = []
categories = []
# Perform Tokenization
for idx, rw in aug_mcommentDF.iterrows():
    comments = rw["stemmed"]
    rating = rw["Sentiment"]
    categories.append(rating)
    movie_comment_aug.append(comments)

"""***Get IDs***"""

ktoken = Tokenizer(lower=True)
ktoken.fit_on_texts(movie_comment_aug)
seq_utterance = ktoken.texts_to_sequences(movie_comment_aug)

"""***Padding***"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

MLEN = 50
# Create pad utterance sequence object
ps_utterance = pad_sequences(seq_utterance, MLEN, padding="post")
ps_utterance = np.array(ps_utterance)
ps_utterance = scaler.fit_transform(ps_utterance)
catlist = np.array(categories)
catlist = catlist.reshape(catlist.shape[0] , 1)
print(catlist.shape)

"""***Splitting into Train and Test set***"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(ps_utterance, catlist, random_state=1, test_size=0.1, shuffle=True)
y_train = np.reshape(y_train, (y_train.shape[0],))
y_test = np.reshape(y_test, (y_test.shape[0]))

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""## **Training Models**

### **Deep Learning**

***Model 1***
"""

utterance_input = layers.Input(shape=X_train[0].shape)
embedding = layers.Embedding(input_dim = len(ktoken.word_index)+1, output_dim = 100)(utterance_input)
LSTM_layer = layers.LSTM(units=256)(embedding)
outlayer = layers.Dense(128, activation='relu')(LSTM_layer)
outlayer = layers.Dense(64, activation='relu')(outlayer)
outlayer = layers.Dense(1, activation='sigmoid')(outlayer)
imdb_mdl = models.Model(inputs=[utterance_input],outputs=[outlayer])
imdb_mdl.summary()

lr = 0.0001
optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)
loss = "binary_crossentropy"
metrics = ['accuracy']
imdb_mdl.compile(optimizer=optimizer, loss="binary_crossentropy", metrics=metrics)

history = imdb_mdl.fit(x=X_train,
                       y=y_train,
                       batch_size=128,
                       epochs=30,
                       validation_split=0.1)

imdb_mdl.evaluate(X_test, y_test)

"""***Model 2***"""

utterance_input = layers.Input(shape=X_train[0].shape)
x = layers.Conv1D(32, 3, padding='same')(utterance_input)
x = layers.Conv1D(64, 3, padding='same')(x)
x = layers.LSTM(units=256)(x)
outlayer = layers.Dense(128, activation='relu')(LSTM_layer)
outlayer = layers.Dense(64, activation='relu')(outlayer)
outlayer = layers.Dense(1, activation='sigmoid')(outlayer)
imdb_mdl2 = models.Model(inputs=[utterance_input],outputs=[outlayer])
imdb_mdl2.summary()